# Assignment2 -HMM

The process of speech recognition can be regarded as a Markov process, and its current state is only related to the previous state.

**Hidden Markov Model** (Hidden Markov Model, HMM) is statistics, which is used to describe a Markov process with hidden unknown parameters. The difficulty is to determine the hidden parameters of the process from the observable parameters. Then use these parameters for further analysis. Hidden Markov model is a kind of Markov chain. Its state cannot be directly observed, but it can be observed through a sequence of observation vectors. Each observation vector is expressed in various states through certain probability density distributions. An observation vector is generated by a sequence of states with a corresponding probability density distribution. Therefore, the hidden Markov model is a double random process-a hidden Markov chain with a certain number of states and a set of explicit random functions.

After using the hidden Markov model, we need to use the corresponding algorithm to solve the following three problems:
1. Likelihood computation ： forward algorithm
2. Finding the most probable state sequence： Viterbi algorithm
3. Estimating the parameters ： forward-backward and EM algorithms

## Main files and main functions

We divide the file into three parts, `generate.py`, `HMM.py` and `main.py`.

#### generate.py

The `generate` part mainly implements the reading and processing of the data set, the generation of the training set list, and the generation of the test set list.

- Data set reading and processing
```python
def generate_mfcc_samples(read_dir = "wav", write_dir = "mfcc"):
    if not os.path.exists(write_dir):
        os.mkdir(write_dir)

    for files in os.listdir(read_dir):
        for file in os.listdir(os.path.join(read_dir, files)):
            if not os.path.exists(os.path.join(write_dir, files)):
                os.mkdir(os.path.join(write_dir, files))
        
            origin_file_path = os.path.join(read_dir,files,file)
            mfcc_file_path = os.path.join(write_dir,files,file.split('.')[0]) + ".npy"
            fs, audio = wav.read(origin_file_path)
            mfcc_feature = mfcc(audio,samplerate=fs)
            delta_mfcc1 = delta(mfcc_feature,1)
            delta_mfcc2 = delta(mfcc_feature,2)
            mfcc_features = np.hstack((mfcc_feature,delta_mfcc1,delta_mfcc2))
            np.save(mfcc_file_path, mfcc_features.T)
```

Read in all the data in the `generate_mfcc_samples` function, and process it into the `mfcc` feature, and carry out the difference processing. The final generated data is transposed to form a with a dimension of (39,) `features`, use `numpy` to save and wait for subsequent processing.

The `generate_my_mfcc_samples` function is a function for extracting the features of `mfcc` which is handwritten by myself. In fact, it is now in `speech.py`.

#### HMM.py

The `HMM` part manually implements the hidden Markov model, encapsulates the hidden Markov model into a class, and operates on it.

In this implementation, all the algorithms used by `HMM` have performed the `log` operation.

- Training model
```python
    def train(self, training_file_list = 'lists/trainingfile_list.npy', epoch = 2):
        ···
		with concurrent.futures.ProcessPoolExecutor(max_workers=15) as executor:
	        for res in tqdm(executor.map(HMM.muti_thread_FR, mean_arr, var_arr, Aij_arr, filename_arr, k_arr), total=len(k_arr), desc=f'epoch {iter}'):
	             mean_numerator, var_numerator, aij_numerator, denominator, log_likelihood_i, likelihood_i, k = res
	             sum_mean_numerator[:,:,k] += mean_numerator[:,1:-1]
	             sum_var_numerator[:,:,k] += var_numerator[:,1:-1]
	             sum_aij_numerator[:,:,k] += aij_numerator[1:-1,1:-1]
	             sum_denominator[:,k] += denominator[1:-1]
	
	             log_likelihood += log_likelihood_i
	             likelihood += likelihood_i
	     ···

```

`aij`：State transition matrix

`mean`：Mean value, used to calculate Gaussian function

`var`：Variance, used to calculate Gaussian function

Model training is iterated by using the `EM` algorithm, and its implementation function is shown below.

- `EM` algorithm for training

```python
    def muti_thread_FR(mean, var, aij, obs, k):
        ···
        for i in range(N):
            log_alpha[i, 0] = np.log(aij[0, i]) + HMM.logGaussian(mean[:,i], var[:,i], obs[:,0])
    
        for t in range(1, length):
            for j in range(1, N-1):
                log_alpha[j, t] = HMM.log_sum_alpha(log_alpha[1:N-1, t-1], aij[1:N-1, j]) + HMM.logGaussian(mean[:,j],var[:,j],obs[:,t])
        log_alpha[N-1,length] = HMM.log_sum_alpha(log_alpha[1:N-1,length-1], aij[1:N-1,N-1])
    
        log_beta[:, length-1] = np.log(aij[:, N-1])
        for t in range(length-2, -1, -1):
            for i in range(1, N-1):
                log_beta[i,t] = HMM.log_sum_beta(aij[i,1:N-1],mean[:,1:N-1],var[:,1:N-1],obs[:,t+1],log_beta[1:N-1,t+1])
        log_beta[N-1,0] = HMM.log_sum_beta(aij[0,1:N-1],mean[:,1:N-1],var[:,1:N-1],obs[:,0],log_beta[1:N-1,0])
    
        log_Xi = np.full((N,N,length), -np.inf)
        for t in range(length-1):
            for j in range(1, N-1):
                for i in range(1, N-1):
                    log_Xi[i,j,t] = log_alpha[i,t] + np.log(aij[i,j]) + HMM.logGaussian(mean[:,j],var[:,j],obs[:,t+1]) + log_beta[j,t+1] - log_alpha[N-1,length]
        
        for i in range(N):
            log_Xi[i,N-1,length-1] = log_alpha[i,length-1] + np.log(aij[i,N-1]) - log_alpha[N-1, length]
        
        log_gamma = np.full((N,length), -np.inf)
        for t in range(length):
            for i in range(1, N-1):
                log_gamma[i,t] = log_alpha[i,t] + log_beta[i,t] - log_alpha[N-1,length]
        gamma = np.exp(log_gamma)
    
        mean_numerator = np.zeros((dim, N))
        var_numerator = np.zeros((dim, N))
        denominator = np.zeros((N))
        aij_numerator = np.zeros((N, N))
        for j in range(1, N-1):
            for t in range(length):
                mean_numerator[:,j] = mean_numerator[:,j] + gamma[j,t]*obs[:,t]
                var_numerator[:,j] = var_numerator[:,j] + gamma[j,t]*np.square(obs[:,t])
                denominator[j] = denominator[j] + gamma[j,t]
        for i in range(1, N-1):
            for j in range(1, N-1):
                for t in range(length):
                    aij_numerator[i,j] = aij_numerator[i,j] + np.exp(log_Xi[i,j,t])
        
        log_likelihood = log_alpha[N-1, length]
        likelihood = np.exp(log_likelihood)
    
        return mean_numerator, var_numerator, aij_numerator, denominator, log_likelihood, likelihood, k
```

Calculation of $\alpha$： $α_t^j=p(x_1,x_2,…,x_t,q_t=j)$  Using forward algorithm

Calculation of $\beta$：$β_t^j=p(x_{t+1},…,x_T |q_t=j)$   Using backward algorithm

Calculation of $\gamma$：$γ_t^j=p(q_t=j│X)=\frac {α_t^j β_t^j}{p(X)} =\frac {α_t^j β_t^j}{∑_{j=1}^Jα_t^j β_t^j} $

Calculation of $\xi$：$ξ_t^{(i,j)} =\frac {(α_t^i a_{ij} b_j (x_{t+1} ) β_{t+1}^j)}{∑_{j=1}^J α_t^j β_t^j}$

Update `aij` based on the above calculation results.$a ̂_{ij}=\frac {∑_{t=1}^{T-1}ξ_t^{(i,j)} ) }{∑_{t=1}^{T-1}γ_t^j }$

Calculate `mean` and `var` and `likelihood`.

- Test model
```python
    def test(self,testing_file_list = 'lists/testingfile_list.npy'):
        testingfile = np.load(testing_file_list)  
        num_of_error = 0
        num_of_testing = testingfile.shape[0]

        #single thread
        # for u in tqdm(testingfile, desc='test'):
        #     k = int(u[0]) - 1
        #     filename = u[1]
        #     features = np.load(filename)
        #     fopt_max = -np.inf
        #     digit = -1
        
        #     for p in range(self.num_of_model):
        #         fopt = HMM.single_thread_viterbi_dist_FR(self.mean[:,:,p], self.var[:,:,p], self.Aij[:,:,p], features) 
        #         if fopt > fopt_max:
        #             digit = p
        #             fopt_max = fopt
        #     if digit != k: 
        #         num_of_error += 1
        
        # muti thread
        filename_arr = testingfile[:,1]
        k_arr = [int(file[0])-1 for file in testingfile]
        
        with concurrent.futures.ProcessPoolExecutor(max_workers=15) as executor:
            for res in tqdm(executor.map(self.muti_thread_test,filename_arr, k_arr), total=len(k_arr), desc='test'):
                num_of_error += res
        
        accuracy_rate = (num_of_testing - num_of_error) * 100 / num_of_testing
        return accuracy_rate


```

The model test uses the `viterbi` algorithm to iterate and select the highest probability as the result. The implementation function is shown below.

- `viterbi` algorithm
```python
    @staticmethod
    def single_thread_viterbi_dist_FR(mean, var, aij, obs):
        dim, t_len = obs.shape
        mean = np.hstack((np.full((dim,1),0),mean,np.full((dim,1),0)))
        var = np.hstack((np.full((dim,1),0),var,np.full((dim,1),0)))
        aij[-1, -1] = 1
        m_len = mean.shape[1] 
        fjt = np.full((m_len, t_len), -np.inf)

        for j in range(1, m_len-1):
            fjt[j,0] = np.log(aij[0,j]) + HMM.logGaussian(mean[:,j],var[:,j], obs[:,0])

        
        for t in range(1, t_len):
            for j in range(1, m_len - 1):
                f_max, i_max, f = -np.inf, -1, -np.inf
                for i in range(1, j+1):
                    if fjt[i, t-1] > -np.inf and aij[i, j] > 0:
                        f = fjt[i,t-1] + np.log(aij[i,j]) + HMM.logGaussian(mean[:,j],var[:,j],obs[:,t])
                    if f > f_max: 
                        f_max = f
                        i_max = i 
                if i_max != -1:

                    fjt[j, t] = f_max
        fopt = -np.inf
        for i in range(1, m_len - 1):
            f = fjt[i, t_len-1] + np.log(aij[i, m_len-1])
            if f > fopt:
                fopt = f
        return fopt
```
Iterate over all states, save only the state with the largest probability each time, and return the explicit state with the largest probability, and compare with the result to determine whether it is correct.

#### main.py

All program calls are implemented in the `main` function, and the model can be directly generated and tested.

## Performance

- `mfcc` data generation

  | my_mfcc/s          | mfcc from python_speech_features/s |
  | ------------------ | ---------------------------------- |
  | 13.783525466918945 | 10.015157222747803                 |

- Single-threaded multi-threaded speed comparison

  | num_of_state | Single-threaded（s/epoch） | multi-threaded（s/epoch） |
  | ------------ | -------------------------- | ------------------------- |
  | 12           | 138                        | 32                        |
  | 13           | 160                        | 36                        |
  | 14           | 190                        | 42                        |
  | 15           | 224                        | 46                        |
  
- test accuracy of different mfcc for 2 epoch

  | num_of_state | my mfcc           | mfcc from python_speech_features |
  | ------------ | ----------------- | -------------------------------- |
  | 12           | 98.53896103896103 | 99.43181818181819                |
  | 13           | 98.45779220779221 | 99.51298701298701                |
  | 14           | 98.45779220779221 | 99.51298701298701                |
  | 15           | 98.45779220779221 | 99.35064935064935                |

- test accuracy of different mfcc for 20 epoch
  | num_of_state | my mfcc           | mfcc from python_speech_features |
  | ------------ | ----------------- | -------------------------------- |
  | 12           | 98.78246753246754 | 99.59415584415585                |
  | 13           | 98.78246753246754 | 99.67532467532467                |
  | 14           | 98.45779220779221 | 99.51298701298701                |
  | 15           | 98.53896103896103 | 99.51298701298701                |
<div STYLE="page-break-after: always;"></div>

- likelihood of different mfcc

![log_likelihood](https://gitee.com/shotray/img-host/raw/master/20211125214355.png)

![my_log_likelihood](https://gitee.com/shotray/img-host/raw/master/20211125214403.png)